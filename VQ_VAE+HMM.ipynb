{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df411bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from notebook_helpers import RandomChunkDataset\n",
    "import pandas as pd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c482561",
   "metadata": {},
   "source": [
    "# VI (Categorical) + HMM ELBO â€” Structured by Components\n",
    "\n",
    "## Encoder (produces logits and posterior)\n",
    "\n",
    "At each time step \\(t\\), the encoder outputs logits $$\\ell_t \\in \\mathbb{R}^K$$ and a categorical posterior:\n",
    "\n",
    "$$\n",
    "q_\\phi(z_t = k \\mid x_{1:T}) = \\mathrm{softmax}(\\ell_t)_k\n",
    "\\quad\\Rightarrow\\quad\n",
    "q_{t,k} = \\frac{e^{\\ell_{t,k}}}{\\sum_{j=1}^K e^{\\ell_{t,j}}}.\n",
    "$$\n",
    "\n",
    "Collecting over time (mean-field form):\n",
    "\n",
    "$$\n",
    "q_\\phi(z_{1:T} \\mid x_{1:T}) = \\prod_{t=1}^T q_\\phi(z_t \\mid x_{1:T}), \n",
    "\\qquad\n",
    "q \\in \\mathbb{R}^{T \\times K}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Prior (HMM: initial distribution and transitions)\n",
    "\n",
    "Initial state distribution:\n",
    "\n",
    "$$\n",
    "p_\\theta(z_1 = k) = \\pi_k, \n",
    "\\qquad \n",
    "\\sum_{k=1}^K \\pi_k = 1.\n",
    "$$\n",
    "\n",
    "Transitions (stationary or input-conditioned):\n",
    "\n",
    "$$\n",
    "p_\\theta(z_t = j \\mid z_{t-1} = i,\\, u_{1:T}) = A_t[i,j],\n",
    "\\qquad\n",
    "\\sum_{j=1}^K A_t[i,j] = 1 \\ \\text{for each row } i.\n",
    "$$\n",
    "\n",
    "Mean-field expected log prior under \\(q\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q[\\log p_\\theta(z_{1:T} \\mid u_{1:T})]\n",
    "=\n",
    "\\sum_{k=1}^K q_{1,k}\\, \\log \\pi_k\n",
    "\\;+\\;\n",
    "\\sum_{t=2}^T \\sum_{i=1}^K \\sum_{j=1}^K q_{t-1,i}\\, q_{t,j}\\, \\log A_t[i,j].\n",
    "$$\n",
    "\n",
    "Parameterization via logits (normalization):\n",
    "\n",
    "$$\n",
    "\\pi = \\mathrm{softmax}(\\alpha), \\quad \\alpha \\in \\mathbb{R}^K,\n",
    "\\qquad\n",
    "A_t[i,\\cdot] = \\mathrm{softmax}\\big(M_t[i,\\cdot]\\big).\n",
    "$$\n",
    "\n",
    "Stationary:\n",
    "$$\n",
    "M_t \\equiv M \\in \\mathbb{R}^{K\\times K}.\n",
    "$$\n",
    "\n",
    "Input-conditioned: \n",
    "$$\n",
    "M_t = g_\\theta^{\\text{trans}}(u_t) \\in \\mathbb{R}^{K\\times K}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Decoder (Gaussian emissions via state embedding)\n",
    "\n",
    "State embedding matrix $$E \\in \\mathbb{R}^{K \\times D}$$ and per-time embedding:\n",
    "\n",
    "$$\n",
    "e_t = q_t^\\top E \\in \\mathbb{R}^D,\n",
    "\\qquad\n",
    "e \\in \\mathbb{R}^{T \\times D}.\n",
    "$$\n",
    "\n",
    "Emission parameters (diagonal Gaussian):\n",
    "\n",
    "$$\n",
    "(\\mu_t,\\, \\log \\sigma_t^2) = g_\\theta(e_t),\n",
    "\\qquad\n",
    "\\mu_t,\\, \\sigma_t \\in \\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "Per-time log likelihood (diagonal Gaussian):\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x_t \\mid z_t) \\approx \\log \\mathcal{N}\\!\\big(x_t;\\, \\mu_t,\\, \\mathrm{diag}(\\sigma_t^2)\\big)\n",
    "= -\\tfrac{1}{2}\\!\\left[\n",
    "d\\log(2\\pi)\n",
    "+ \\sum_{j=1}^d \\log \\sigma^2_{t,j}\n",
    "+ \\sum_{j=1}^d \\frac{(x_{t,j}-\\mu_{t,j})^2}{\\sigma^2_{t,j}}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Expected reconstruction term under \\(q\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q[\\log p_\\theta(x_{1:T} \\mid z_{1:T})]\n",
    "=\n",
    "\\sum_{t=1}^T \\sum_{k=1}^K q_{t,k}\\, \\log p_\\theta(x_t \\mid z_t = k)\n",
    "\\;\\;\\approx\\;\\;\n",
    "\\sum_{t=1}^T \\log \\mathcal{N}\\!\\big(x_t;\\, \\mu_t,\\, \\mathrm{diag}(\\sigma_t^2)\\big).\n",
    "$$\n",
    "\n",
    "(The approximation uses $e_t = q_t^\\top E$ to condition $\\mu_t,\\sigma_t$ directly on $q_t$.)\n",
    "\n",
    "---\n",
    "\n",
    "## Main Model (ELBO, entropy, and loss)\n",
    "\n",
    "Entropy (sum over time and states):\n",
    "\n",
    "$$\n",
    "-\\mathbb{E}_q[\\log q_\\phi(z_{1:T} \\mid x_{1:T})]\n",
    "=\n",
    "\\sum_{t=1}^T \\sum_{k=1}^K \\big(- q_{t,k}\\, \\log q_{t,k}\\big).\n",
    "$$\n",
    "\n",
    "Full ELBO:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi)\n",
    "=\n",
    "\\underbrace{\\mathbb{E}_q[\\log p_\\theta(x_{1:T} \\mid z_{1:T})]}_{\\text{reconstruction}}\n",
    "+\n",
    "\\underbrace{\\mathbb{E}_q[\\log p_\\theta(z_{1:T} \\mid u_{1:T})]}_{\\text{HMM prior}}\n",
    "-\n",
    "\\underbrace{\\mathbb{E}_q[\\log q_\\phi(z_{1:T} \\mid x_{1:T})]}_{\\text{entropy}}.\n",
    "$$\n",
    "\n",
    "Optional $\\beta$-weighting (warm-up):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\beta(\\theta,\\phi)\n",
    "=\n",
    "\\mathrm{Recon}\n",
    "+\n",
    "\\beta \\,(\\mathrm{Prior} - \\mathrm{Entropy}),\n",
    "\\qquad\n",
    "\\beta \\in [0,1].\n",
    "$$\n",
    "\n",
    "Training objective (minimize negative ELBO):\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta,\\phi) = -\\,\\mathcal{L}_\\beta(\\theta,\\phi).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on symbols\n",
    "\n",
    "- $K$: number of discrete hidden states (regimes).\n",
    "- $T$: number of time steps.\n",
    "- $d$: data dimension per time step.\n",
    "- $q_{t,k}$: variational posterior probability of state $k$ at time $t$.\n",
    "- $\\pi$: initial state distribution (with $\\sum_{k=1}^K \\pi_k = 1$).\n",
    "- $A_t$: transition probabilities at time $t$ (each row sums to $1$).\n",
    "- $E \\in \\mathbb{R}^{K\\times D}$: state embedding matrix; $D$ is embedding size.\n",
    "- $g_\\theta$: decoder network producing $(\\mu_t,\\log\\sigma_t^2)$ from $e_t$.\n",
    "- $u_t$: optional exogenous inputs for input-conditioned transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "617fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, K):\n",
    "        super().__init__()\n",
    "        # x = (batch_size, input_dim, T)\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim2, kernel_size=3, padding=1)\n",
    "        self.to_logits = nn.Conv1d(hidden_dim2, K, kernel_size=1)\n",
    "        # logits = (batch_size, K, T)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.to_logits(x) # (B, K, T)\n",
    "        return logits\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, K, u_dim=None, trans_hidden=128):\n",
    "        super().__init__()\n",
    "        self.k_clusters = K\n",
    "        self.u_dim = u_dim\n",
    "        # initial state logits (unnormalized) with softmax -> pi (normalized state distribtuion)\n",
    "        self.log_prior = nn.Parameter(torch.zeros(K), requires_grad=True)\n",
    "\n",
    "        if u_dim is None:\n",
    "            raise ValueError('Not supporting stationary transitions')\n",
    "        else:\n",
    "            # input-conditioned transitions: small MLP maps u_t -> K*K logits\n",
    "            self.transition_net = nn.Sequential(\n",
    "                nn.Linear(u_dim, trans_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(trans_hidden, K * K)\n",
    "            )\n",
    "\n",
    "    def forward(self, u=None):\n",
    "        # pi = initial state distribution\n",
    "        log_pi = F.log_softmax(self.log_prior, dim=-1)\n",
    "\n",
    "        if self.u_dim is None or u is None:\n",
    "            raise ValueError('Not supporting stationary transitions')\n",
    "\n",
    "        # input-conditioned case\n",
    "        if u.dim() == 3 and u.shape[1] == self.u_dim:\n",
    "            # (B, U_dim, T) -> (B, T, U_dim)\n",
    "            u = u.permute(0, 2, 1)\n",
    "\n",
    "        B, T, U = u.shape\n",
    "        u_flat = u.reshape(B * T, U)\n",
    "        logits = self.transition_net(u_flat) # (B*T, K*K)\n",
    "        logits = logits.view(B, T, self.k_clusters, self.k_clusters)\n",
    "        # normalize by row \n",
    "        log_A = F.log_softmax(logits, dim=-1)\n",
    "        return log_pi, log_A\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, K, latent_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.latent_dim = latent_dim\n",
    "        self.E = nn.Embedding(K, latent_dim)  # embedding for each discrete state\n",
    "        self.conv1 = nn.Conv1d(latent_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.to_output = nn.Conv1d(hidden_dim, output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, q):\n",
    "        # q: (B, T) discrete latent states\n",
    "        B, K, T = q.shape\n",
    "        e = torch.matmul(q.permute(0, 2, 1), self.E.weight)\n",
    "        e_t = e.permute(0, 2, 1)\n",
    "\n",
    "        x = self.conv1(e_t)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        recon_x = self.to_output(x)\n",
    "        return recon_x\n",
    "    \n",
    "\n",
    "class VAE_HMM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, K, hidden_dim2, u_dim=None, trans_hidden=128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, hidden_dim2, K)\n",
    "        self.prior = Prior(K, u_dim=u_dim, trans_hidden=trans_hidden)\n",
    "        self.decoder = Decoder(K, latent_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=input_dim)\n",
    "        self.K = K\n",
    "    \n",
    "    def encode(self, x):\n",
    "        logits = self.encoder(x)\n",
    "        return logits\n",
    "    \n",
    "    def decode(self, q):\n",
    "        recon_x = self.decoder(q) \n",
    "        return recon_x\n",
    "    \n",
    "    def compute_loss(self, x, u=None, lengths=None, beta = 1.0):\n",
    "        # ELBO loss = reconstruction loss + HMM prior loss - entropy of q -> minimize negative elbo\n",
    "\n",
    "        B, C, T = x.shape\n",
    "        # mask of valid timesteps (B: batch size, T)\n",
    "        if lengths is None:\n",
    "            raise ValueError('lengths must be provided')\n",
    "        mask = (torch.arange(T, device=x.device)[None, :] < lengths[:, None].to(x.device))\n",
    "\n",
    "        log_pi, log_A = self.prior(u)\n",
    "        logits = self.encoder(x)  # (B, K, T)\n",
    "        q_probs = F.softmax(logits, dim=1)  # (B, K, T)\n",
    "        recon_x = self.decode(q_probs)\n",
    "\n",
    "        # reconstruction loss (sum over data dim, sum over valid timesteps, average over batch)\n",
    "        per_t_sq = ((recon_x - x) ** 2).sum(dim=1) # (B, T) \n",
    "        recon_loss = (per_t_sq * mask.float()).sum() / B\n",
    "\n",
    "        # HMM prior loss\n",
    "        prior_loss = 0.0\n",
    "        q1 = q_probs[:, :, 0]\n",
    "        initial_term = (q1 * log_pi.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "        # reshape to get q_t-1 and q_t\n",
    "        q_root = q_probs[:, :, :-1]\n",
    "        q_transition = q_probs[:, :, 1:]\n",
    "\n",
    "        # reshape to (B, T-1, K, 1) and (B, T-1, 1, K) and (B, T-1, K, K)\n",
    "        q_root = q_root.permute(0, 2, 1).unsqueeze(-1)\n",
    "        q_transition = q_transition.permute(0, 2, 1).unsqueeze(-2)\n",
    "        logA_bt = log_A[:, 1:, :, :]\n",
    "\n",
    "        joint = q_root * q_transition * logA_bt # (B, T-1, K, K)\n",
    "        trans_terms_bt = joint.sum(dim=(2, 3))\n",
    "        trans_mask = (mask[:, 1:] & mask[:, :-1]).float()\n",
    "        trans_term = (trans_terms_bt * trans_mask).sum(dim=1) \n",
    "\n",
    "        prior_per_batch = initial_term + trans_term\n",
    "        prior_loss = - prior_per_batch.mean()\n",
    "\n",
    "        # entropy (sum over time and states (T, K) -q_t,k log q_t,k) averaged over batch, for only valid timesteps\n",
    "        log_q = F.log_softmax(logits, dim=1)  # (batch size, K, T)\n",
    "        # compute per-(B,T)\n",
    "        per_bt_entropy = - (q_probs * log_q).sum(dim=1)  # (batch size, T)\n",
    "        entropy = (per_bt_entropy * mask.float()).sum() / B\n",
    "\n",
    "        return recon_loss + beta * (prior_loss - entropy)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.encode(x)\n",
    "        q = F.softmax(logits, dim=1)\n",
    "        recon_x = self.decode(q)\n",
    "        return recon_x, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "617fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        # increase beta for first half of epochs gradually to 1\n",
    "        beta = min(1.0, 2.0 * (epoch + 1) / num_epochs)\n",
    "        for batch in dataloader:\n",
    "            x, u, lengths = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(x, u, lengths, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "162b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x_seqs = [item[0] for item in batch]\n",
    "    u_seqs = [item[1] for item in batch]\n",
    "\n",
    "    lengths = torch.tensor([int(item[2]) for item in batch], dtype=torch.long)\n",
    "    max_len = lengths.max().item()\n",
    "    B = len(batch)\n",
    "    C = x_seqs[0].shape[0]\n",
    "    U = u_seqs[0].shape[0]\n",
    "\n",
    "    x_batch = torch.zeros(B, C, max_len, device=device)\n",
    "    u_batch = torch.zeros(B, U, max_len, device=device)\n",
    "\n",
    "    for i in range(B):\n",
    "        L = lengths[i].item()\n",
    "        x_batch[i, :, :L] = x_seqs[i]\n",
    "        u_batch[i, :, :L] = u_seqs[i]\n",
    "\n",
    "    return x_batch, u_batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "829f5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = pd.read_csv('train_dataset_scaled.csv').drop(columns=['date']).values\n",
    "u_data = pd.read_csv('train_dataset_scaled.csv').drop(columns=['date', 'historical_vol']).values\n",
    "\n",
    "x_sequences = torch.tensor(x_data, dtype=torch.float)\n",
    "u_sequences = torch.tensor(u_data, dtype=torch.float)\n",
    "\n",
    "x_sequences = [x_sequences.permute(1,0)]\n",
    "u_sequences = [u_sequences.permute(1,0)]\n",
    "\n",
    "\n",
    "dataset = RandomChunkDataset(x_sequences, u_sequences, min_len=20, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 532.2815\n",
      "Epoch 2/150, Loss: 516.2423\n",
      "Epoch 2/150, Loss: 516.2423\n",
      "Epoch 3/150, Loss: 526.5152\n",
      "Epoch 3/150, Loss: 526.5152\n",
      "Epoch 4/150, Loss: 527.0410\n",
      "Epoch 4/150, Loss: 527.0410\n",
      "Epoch 5/150, Loss: 487.1830\n",
      "Epoch 5/150, Loss: 487.1830\n",
      "Epoch 6/150, Loss: 432.1284\n",
      "Epoch 6/150, Loss: 432.1284\n",
      "Epoch 7/150, Loss: 384.0841\n",
      "Epoch 7/150, Loss: 384.0841\n",
      "Epoch 8/150, Loss: 349.2764\n",
      "Epoch 8/150, Loss: 349.2764\n",
      "Epoch 9/150, Loss: 297.2445\n",
      "Epoch 9/150, Loss: 297.2445\n",
      "Epoch 10/150, Loss: 280.9104\n",
      "Epoch 10/150, Loss: 280.9104\n",
      "Epoch 11/150, Loss: 248.7507\n",
      "Epoch 11/150, Loss: 248.7507\n",
      "Epoch 12/150, Loss: 230.6467\n",
      "Epoch 12/150, Loss: 230.6467\n",
      "Epoch 13/150, Loss: 207.9305\n",
      "Epoch 13/150, Loss: 207.9305\n",
      "Epoch 14/150, Loss: 184.8224\n",
      "Epoch 14/150, Loss: 184.8224\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
    "u_dim = u_sequences[0].shape[0]\n",
    "C = x_sequences[0].shape[0]\n",
    "num_clusters = 3\n",
    "hidden0 = 64\n",
    "hidden1 = 64\n",
    "\n",
    "model = VAE_HMM(input_dim=C, hidden_dim=hidden0, K=num_clusters, hidden_dim2=hidden1, u_dim=u_dim, trans_hidden=64)\n",
    "trained = train_model(model, dataloader, num_epochs=150, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc1c586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder path after training\n",
    "encoder_path = 'encoder_saved.pth'\n",
    "torch.save({'model_state_dict': trained.encoder.state_dict(),\n",
    "            'config': {\n",
    "                'input_dim': C,\n",
    "                'hidden_dim': hidden0,\n",
    "                'hidden_dim2': hidden1,\n",
    "                'K': num_clusters\n",
    "            }\n",
    "           }, encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b77c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape (K, T): (5, 100)\n",
      "regimes.shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "end_day = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x_data = pd.read_csv('test_data.csv').drop(columns=['Date']).values[:end_day]\n",
    "u_data = pd.read_csv('test_data.csv').drop(columns=['Date', 'historical_vol']).values[:end_day]\n",
    "\n",
    "ckpt = torch.load('encoder_saved.pth', map_location='cpu')\n",
    "enc_cfg = ckpt.get('config', {})\n",
    "input_dim = enc_cfg.get('input_dim', C)\n",
    "hidden_dim = enc_cfg.get('hidden_dim', 32)\n",
    "hidden_dim2 = enc_cfg.get('hidden_dim2', hidden_dim)\n",
    "K = enc_cfg.get('K', num_clusters)\n",
    "\n",
    "# Recreate encoder architecture\n",
    "encoder_loaded = Encoder(input_dim, hidden_dim, hidden_dim2, K)\n",
    "encoder_loaded.load_state_dict(ckpt['model_state_dict'])\n",
    "encoder_loaded.to(device)\n",
    "encoder_loaded.eval()\n",
    "\n",
    "x_tensor = torch.tensor(x_data, dtype=torch.float).permute(1, 0).unsqueeze(0)  # (1, C, T)\n",
    "x_tensor = x_tensor.to(device)\n",
    "\n",
    "encoder_loaded.to(device)\n",
    "encoder_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    logits = encoder_loaded(x_tensor)                 # (1, K, T)\n",
    "    probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # (K, T)\n",
    "    regimes = np.argmax(probs, axis=0)                # (T,)\n",
    "\n",
    "print('probs.shape (K, T):', probs.shape)\n",
    "print('regimes.shape:', regimes.shape)  # should equal T_full\n",
    "\n",
    "# align with SP500 history (example)\n",
    "import yfinance as yf\n",
    "sp500 = yf.Ticker(\"^GSPC\")\n",
    "sp500_data = sp500.history(start=\"2020-01-01\", end=\"2024-12-31\")\n",
    "prices = sp500_data['Close'].values[:end_day]\n",
    "\n",
    "\n",
    "prices_aligned = prices[:len(regimes)]\n",
    "index_aligned = sp500_data.index[:len(regimes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1421d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatorai25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
