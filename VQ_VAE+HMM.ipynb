{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0df411bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from notebook_helpers import RandomChunkDataset\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c482561",
   "metadata": {},
   "source": [
    "# VI (Categorical) + HMM ELBO â€” Structured by Components\n",
    "\n",
    "## Encoder (produces logits and posterior)\n",
    "\n",
    "At each time step \\(t\\), the encoder outputs logits $$\\ell_t \\in \\mathbb{R}^K$$ and a categorical posterior:\n",
    "\n",
    "$$\n",
    "q_\\phi(z_t = k \\mid x_{1:T}) = \\mathrm{softmax}(\\ell_t)_k\n",
    "\\quad\\Rightarrow\\quad\n",
    "q_{t,k} = \\frac{e^{\\ell_{t,k}}}{\\sum_{j=1}^K e^{\\ell_{t,j}}}.\n",
    "$$\n",
    "\n",
    "Collecting over time (mean-field form):\n",
    "\n",
    "$$\n",
    "q_\\phi(z_{1:T} \\mid x_{1:T}) = \\prod_{t=1}^T q_\\phi(z_t \\mid x_{1:T}), \n",
    "\\qquad\n",
    "q \\in \\mathbb{R}^{T \\times K}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Prior (HMM: initial distribution and transitions)\n",
    "\n",
    "Initial state distribution:\n",
    "\n",
    "$$\n",
    "p_\\theta(z_1 = k) = \\pi_k, \n",
    "\\qquad \n",
    "\\sum_{k=1}^K \\pi_k = 1.\n",
    "$$\n",
    "\n",
    "Transitions (stationary or input-conditioned):\n",
    "\n",
    "$$\n",
    "p_\\theta(z_t = j \\mid z_{t-1} = i,\\, u_{1:T}) = A_t[i,j],\n",
    "\\qquad\n",
    "\\sum_{j=1}^K A_t[i,j] = 1 \\ \\text{for each row } i.\n",
    "$$\n",
    "\n",
    "Mean-field expected log prior under \\(q\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q[\\log p_\\theta(z_{1:T} \\mid u_{1:T})]\n",
    "=\n",
    "\\sum_{k=1}^K q_{1,k}\\, \\log \\pi_k\n",
    "\\;+\\;\n",
    "\\sum_{t=2}^T \\sum_{i=1}^K \\sum_{j=1}^K q_{t-1,i}\\, q_{t,j}\\, \\log A_t[i,j].\n",
    "$$\n",
    "\n",
    "Parameterization via logits (normalization):\n",
    "\n",
    "$$\n",
    "\\pi = \\mathrm{softmax}(\\alpha), \\quad \\alpha \\in \\mathbb{R}^K,\n",
    "\\qquad\n",
    "A_t[i,\\cdot] = \\mathrm{softmax}\\big(M_t[i,\\cdot]\\big).\n",
    "$$\n",
    "\n",
    "Stationary:\n",
    "$$\n",
    "M_t \\equiv M \\in \\mathbb{R}^{K\\times K}.\n",
    "$$\n",
    "\n",
    "Input-conditioned: \n",
    "$$\n",
    "M_t = g_\\theta^{\\text{trans}}(u_t) \\in \\mathbb{R}^{K\\times K}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Decoder (Gaussian emissions via state embedding)\n",
    "\n",
    "State embedding matrix $$E \\in \\mathbb{R}^{K \\times D}$$ and per-time embedding:\n",
    "\n",
    "$$\n",
    "e_t = q_t^\\top E \\in \\mathbb{R}^D,\n",
    "\\qquad\n",
    "e \\in \\mathbb{R}^{T \\times D}.\n",
    "$$\n",
    "\n",
    "Emission parameters (diagonal Gaussian):\n",
    "\n",
    "$$\n",
    "(\\mu_t,\\, \\log \\sigma_t^2) = g_\\theta(e_t),\n",
    "\\qquad\n",
    "\\mu_t,\\, \\sigma_t \\in \\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "Per-time log likelihood (diagonal Gaussian):\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x_t \\mid z_t) \\approx \\log \\mathcal{N}\\!\\big(x_t;\\, \\mu_t,\\, \\mathrm{diag}(\\sigma_t^2)\\big)\n",
    "= -\\tfrac{1}{2}\\!\\left[\n",
    "d\\log(2\\pi)\n",
    "+ \\sum_{j=1}^d \\log \\sigma^2_{t,j}\n",
    "+ \\sum_{j=1}^d \\frac{(x_{t,j}-\\mu_{t,j})^2}{\\sigma^2_{t,j}}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Expected reconstruction term under \\(q\\):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q[\\log p_\\theta(x_{1:T} \\mid z_{1:T})]\n",
    "=\n",
    "\\sum_{t=1}^T \\sum_{k=1}^K q_{t,k}\\, \\log p_\\theta(x_t \\mid z_t = k)\n",
    "\\;\\;\\approx\\;\\;\n",
    "\\sum_{t=1}^T \\log \\mathcal{N}\\!\\big(x_t;\\, \\mu_t,\\, \\mathrm{diag}(\\sigma_t^2)\\big).\n",
    "$$\n",
    "\n",
    "(The approximation uses $e_t = q_t^\\top E$ to condition $\\mu_t,\\sigma_t$ directly on $q_t$.)\n",
    "\n",
    "---\n",
    "\n",
    "## Main Model (ELBO, entropy, and loss)\n",
    "\n",
    "Entropy (sum over time and states):\n",
    "\n",
    "$$\n",
    "-\\mathbb{E}_q[\\log q_\\phi(z_{1:T} \\mid x_{1:T})]\n",
    "=\n",
    "\\sum_{t=1}^T \\sum_{k=1}^K \\big(- q_{t,k}\\, \\log q_{t,k}\\big).\n",
    "$$\n",
    "\n",
    "Full ELBO:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi)\n",
    "=\n",
    "\\underbrace{\\mathbb{E}_q[\\log p_\\theta(x_{1:T} \\mid z_{1:T})]}_{\\text{reconstruction}}\n",
    "+\n",
    "\\underbrace{\\mathbb{E}_q[\\log p_\\theta(z_{1:T} \\mid u_{1:T})]}_{\\text{HMM prior}}\n",
    "-\n",
    "\\underbrace{\\mathbb{E}_q[\\log q_\\phi(z_{1:T} \\mid x_{1:T})]}_{\\text{entropy}}.\n",
    "$$\n",
    "\n",
    "Optional $\\beta$-weighting (warm-up):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\beta(\\theta,\\phi)\n",
    "=\n",
    "\\mathrm{Recon}\n",
    "+\n",
    "\\beta \\,(\\mathrm{Prior} - \\mathrm{Entropy}),\n",
    "\\qquad\n",
    "\\beta \\in [0,1].\n",
    "$$\n",
    "\n",
    "Training objective (minimize negative ELBO):\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta,\\phi) = -\\,\\mathcal{L}_\\beta(\\theta,\\phi).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on symbols\n",
    "\n",
    "- $K$: number of discrete hidden states (regimes).\n",
    "- $T$: number of time steps.\n",
    "- $d$: data dimension per time step.\n",
    "- $q_{t,k}$: variational posterior probability of state $k$ at time $t$.\n",
    "- $\\pi$: initial state distribution (with $\\sum_{k=1}^K \\pi_k = 1$).\n",
    "- $A_t$: transition probabilities at time $t$ (each row sums to $1$).\n",
    "- $E \\in \\mathbb{R}^{K\\times D}$: state embedding matrix; $D$ is embedding size.\n",
    "- $g_\\theta$: decoder network producing $(\\mu_t,\\log\\sigma_t^2)$ from $e_t$.\n",
    "- $u_t$: optional exogenous inputs for input-conditioned transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, K):\n",
    "        super().__init__()\n",
    "        # x = (batch_size, input_dim, T)\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim2, kernel_size=3, padding=1)\n",
    "        self.to_logits = nn.Conv1d(hidden_dim2, K, kernel_size=1)\n",
    "        # logits = (batch_size, K, T)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.to_logits(x)\n",
    "        return logits\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, K, u_dim=None, trans_hidden=128):\n",
    "        super().__init__()\n",
    "        self.k_clusters = K\n",
    "        self.u_dim = u_dim\n",
    "        # initial state logits (unnormalized) with softmax -> pi (normalized state distribtuion)\n",
    "        self.log_prior = nn.Parameter(torch.zeros(K), requires_grad=True)\n",
    "\n",
    "        if u_dim is None:\n",
    "            raise ValueError('Not supporting stationary transitions')\n",
    "        else:\n",
    "            # input-conditioned transitions: small MLP maps u_t -> K*K logits\n",
    "            self.transition_net = nn.Sequential(\n",
    "                nn.Linear(u_dim, trans_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(trans_hidden, K * K)\n",
    "            )\n",
    "\n",
    "    def forward(self, u=None):\n",
    "        # pi = initial state distribution\n",
    "        log_pi = F.log_softmax(self.log_prior, dim=-1)\n",
    "\n",
    "        if self.u_dim is None or u is None:\n",
    "            raise ValueError('Not supporting stationary transitions')\n",
    "\n",
    "        # input-conditioned case\n",
    "        if u.dim() == 3 and u.shape[1] == self.u_dim:\n",
    "            # (B, U_dim, T) -> (B, T, U_dim)\n",
    "            u = u.permute(0, 2, 1)\n",
    "\n",
    "        B, T, U = u.shape\n",
    "        u_flat = u.reshape(B * T, U)\n",
    "        logits = self.transition_net(u_flat) # (B*T, K*K)\n",
    "        logits = logits.view(B, T, self.k_clusters, self.k_clusters)\n",
    "        # normalize by row \n",
    "        log_A = F.log_softmax(logits, dim=-1)\n",
    "        return log_pi, log_A\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, K, latent_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.latent_dim = latent_dim\n",
    "        self.E = nn.Embedding(K, latent_dim)  # embedding for each discrete state\n",
    "        self.conv1 = nn.Conv1d(latent_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.to_output = nn.Conv1d(hidden_dim, output_dim*2, kernel_size=1)\n",
    "\n",
    "    def forward(self, q):\n",
    "        # q: (B, T) discrete latent states\n",
    "        B, K, T = q.shape\n",
    "        e = torch.matmul(q.permute(0, 2, 1), self.E.weight)\n",
    "        e_t = e.permute(0, 2, 1)\n",
    "\n",
    "        x = self.conv1(e_t)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.to_output(x)\n",
    "\n",
    "        C = out.shape[1] // 2\n",
    "        mu = out[:, :C, :]\n",
    "        logvar = out[:, C:, :]\n",
    "        return mu, logvar\n",
    "    \n",
    "\n",
    "class VAE_HMM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, K, hidden_dim2, u_dim=None, trans_hidden=128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, hidden_dim2, K)\n",
    "        self.prior = Prior(K, u_dim=u_dim, trans_hidden=trans_hidden)\n",
    "        self.decoder = Decoder(K, latent_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=input_dim)\n",
    "        self.K = K\n",
    "    \n",
    "    def encode(self, x):\n",
    "        logits = self.encoder(x)\n",
    "        return logits\n",
    "    \n",
    "    def decode(self, q):\n",
    "        return self.decoder(q)\n",
    "    \n",
    "    def compute_loss(self, x, u=None, lengths=None, beta = 1.0):\n",
    "        # ELBO loss = reconstruction loss + HMM prior loss - entropy of q -> minimize negative elbo\n",
    "\n",
    "        B, C, T = x.shape\n",
    "        # mask of valid timesteps (B: batch size, T)\n",
    "        if lengths is None:\n",
    "            raise ValueError('lengths must be provided')\n",
    "        mask = (torch.arange(T, device=x.device)[None, :] < lengths[:, None].to(x.device))\n",
    "\n",
    "        log_pi, log_A = self.prior(u)\n",
    "        logits = self.encoder(x)  # (B, K, T)\n",
    "        q_probs = F.softmax(logits, dim=1)  # (B, K, T)\n",
    "        mu, logvar = self.decode(q_probs)\n",
    "\n",
    "        # reconstruction loss with negative gaussian log likelihood mse\n",
    "        recon_mask = mask.to(dtype=x.dtype).unsqueeze(1)\n",
    "        squared = ((mu - x) ** 2)\n",
    "        var = logvar.exp()\n",
    "        neg_log_likelihood = 0.5 * (torch.log(2 * math.pi * var) + squared/var) #\n",
    "        nll_masked = (neg_log_likelihood * recon_mask).sum()\n",
    "        num_valid = (mask.sum() * x.shape[1]).clamp(min=1.0)\n",
    "        recon_loss = nll_masked / num_valid\n",
    "\n",
    "        # HMM prior loss\n",
    "        prior_loss = 0.0\n",
    "        q1 = q_probs[:, :, 0] \n",
    "        initial_term = (q1 * log_pi.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "        # reshape to get q_t-1 and q_t\n",
    "        q_root = q_probs[:, :, :-1]\n",
    "        q_transition = q_probs[:, :, 1:]\n",
    "\n",
    "        # reshape to (B, T-1, K, 1) and (B, T-1, 1, K) and (B, T-1, K, K)\n",
    "        q_root = q_root.permute(0, 2, 1).unsqueeze(-1)\n",
    "        q_transition = q_transition.permute(0, 2, 1).unsqueeze(-2)\n",
    "        logA_bt = log_A[:, 1:, :, :]\n",
    "\n",
    "        joint = q_root * q_transition * logA_bt # (B, T-1, K, K)\n",
    "        trans_terms_bt = joint.sum(dim=(2, 3))\n",
    "        trans_mask = (mask[:, 1:] & mask[:, :-1]).float()\n",
    "        trans_term = (trans_terms_bt * trans_mask).sum(dim=1) \n",
    "\n",
    "        prior_per_batch = initial_term + trans_term\n",
    "        prior_loss = - prior_per_batch.mean()\n",
    "\n",
    "        # entropy (sum over time and states (T, K) -q_t,k log q_t,k) averaged over batch, for only valid timesteps\n",
    "        log_q = F.log_softmax(logits, dim=1) # (batch size, K, T)\n",
    "        # compute per batch\n",
    "        per_bt_entropy = - (q_probs * log_q).sum(dim=1)\n",
    "        entropy = (per_bt_entropy * mask.float()).sum() / B\n",
    "\n",
    "        return recon_loss + beta * (prior_loss - entropy)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.encode(x)\n",
    "        q = F.softmax(logits, dim=1)\n",
    "        recon_x = self.decode(q)\n",
    "        return recon_x, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "617fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        # increase beta for first half of epochs gradually to 1\n",
    "        beta = min(1.0, 2.0 * (epoch + 1) / num_epochs)\n",
    "        for batch in dataloader:\n",
    "            x, u, lengths = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(x, u, lengths, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x_seqs = [item[0] for item in batch]\n",
    "    u_seqs = [item[1] for item in batch]\n",
    "\n",
    "    lengths = torch.tensor([int(item[2]) for item in batch], dtype=torch.long)\n",
    "    max_len = lengths.max().item()\n",
    "    B = len(batch)\n",
    "    C = x_seqs[0].shape[0]\n",
    "    U = u_seqs[0].shape[0]\n",
    "\n",
    "    x_batch = torch.zeros(B, C, max_len, device=device)\n",
    "    u_batch = torch.zeros(B, U, max_len, device=device)\n",
    "\n",
    "    for i in range(B):\n",
    "        L = lengths[i].item()\n",
    "        x_batch[i, :, :L] = x_seqs[i]\n",
    "        u_batch[i, :, :L] = u_seqs[i]\n",
    "\n",
    "    return x_batch, u_batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "829f5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = pd.read_csv('train_dataset_scaled.csv').drop(columns=['date']).values\n",
    "u_data = pd.read_csv('train_dataset_scaled.csv').drop(columns=['date', 'historical_vol']).values\n",
    "\n",
    "x_sequences = torch.tensor(x_data, dtype=torch.float)\n",
    "u_sequences = torch.tensor(u_data, dtype=torch.float)\n",
    "\n",
    "x_sequences = [x_sequences.permute(1,0)]\n",
    "u_sequences = [u_sequences.permute(1,0)]\n",
    "\n",
    "\n",
    "dataset = RandomChunkDataset(x_sequences, u_sequences, min_len=20, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e94dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 1.4494\n",
      "Epoch 2/150, Loss: 1.4697\n",
      "Epoch 2/150, Loss: 1.4697\n",
      "Epoch 3/150, Loss: 1.4930\n",
      "Epoch 3/150, Loss: 1.4930\n",
      "Epoch 4/150, Loss: 1.5040\n",
      "Epoch 4/150, Loss: 1.5040\n",
      "Epoch 5/150, Loss: 1.4767\n",
      "Epoch 5/150, Loss: 1.4767\n",
      "Epoch 6/150, Loss: 1.4756\n",
      "Epoch 6/150, Loss: 1.4756\n",
      "Epoch 7/150, Loss: 1.4798\n",
      "Epoch 7/150, Loss: 1.4798\n",
      "Epoch 8/150, Loss: 1.4467\n",
      "Epoch 8/150, Loss: 1.4467\n",
      "Epoch 9/150, Loss: 1.4524\n",
      "Epoch 9/150, Loss: 1.4524\n",
      "Epoch 10/150, Loss: 1.4412\n",
      "Epoch 10/150, Loss: 1.4412\n",
      "Epoch 11/150, Loss: 1.3972\n",
      "Epoch 11/150, Loss: 1.3972\n",
      "Epoch 12/150, Loss: 1.3724\n",
      "Epoch 12/150, Loss: 1.3724\n",
      "Epoch 13/150, Loss: 1.3520\n",
      "Epoch 13/150, Loss: 1.3520\n",
      "Epoch 14/150, Loss: 1.3129\n",
      "Epoch 14/150, Loss: 1.3129\n",
      "Epoch 15/150, Loss: 1.2590\n",
      "Epoch 15/150, Loss: 1.2590\n",
      "Epoch 16/150, Loss: 1.2098\n",
      "Epoch 16/150, Loss: 1.2098\n",
      "Epoch 17/150, Loss: 1.1527\n",
      "Epoch 17/150, Loss: 1.1527\n",
      "Epoch 18/150, Loss: 1.0776\n",
      "Epoch 18/150, Loss: 1.0776\n",
      "Epoch 19/150, Loss: 1.0164\n",
      "Epoch 19/150, Loss: 1.0164\n",
      "Epoch 20/150, Loss: 0.9392\n",
      "Epoch 20/150, Loss: 0.9392\n",
      "Epoch 21/150, Loss: 0.8574\n",
      "Epoch 21/150, Loss: 0.8574\n",
      "Epoch 22/150, Loss: 0.7566\n",
      "Epoch 22/150, Loss: 0.7566\n",
      "Epoch 23/150, Loss: 0.6369\n",
      "Epoch 23/150, Loss: 0.6369\n",
      "Epoch 24/150, Loss: 0.5227\n",
      "Epoch 24/150, Loss: 0.5227\n",
      "Epoch 25/150, Loss: 0.4467\n",
      "Epoch 25/150, Loss: 0.4467\n",
      "Epoch 26/150, Loss: 0.3916\n",
      "Epoch 26/150, Loss: 0.3916\n",
      "Epoch 27/150, Loss: 0.3561\n",
      "Epoch 27/150, Loss: 0.3561\n",
      "Epoch 28/150, Loss: 0.3386\n",
      "Epoch 28/150, Loss: 0.3386\n",
      "Epoch 29/150, Loss: 0.3371\n",
      "Epoch 29/150, Loss: 0.3371\n",
      "Epoch 30/150, Loss: 0.3159\n",
      "Epoch 30/150, Loss: 0.3159\n",
      "Epoch 31/150, Loss: 0.3176\n",
      "Epoch 31/150, Loss: 0.3176\n",
      "Epoch 32/150, Loss: 0.2976\n",
      "Epoch 32/150, Loss: 0.2976\n",
      "Epoch 33/150, Loss: 0.2940\n",
      "Epoch 33/150, Loss: 0.2940\n",
      "Epoch 34/150, Loss: 0.2660\n",
      "Epoch 34/150, Loss: 0.2660\n",
      "Epoch 35/150, Loss: 0.2522\n",
      "Epoch 35/150, Loss: 0.2522\n",
      "Epoch 36/150, Loss: 0.2435\n",
      "Epoch 36/150, Loss: 0.2435\n",
      "Epoch 37/150, Loss: 0.2278\n",
      "Epoch 37/150, Loss: 0.2278\n",
      "Epoch 38/150, Loss: 0.2118\n",
      "Epoch 38/150, Loss: 0.2118\n",
      "Epoch 39/150, Loss: 0.1979\n",
      "Epoch 39/150, Loss: 0.1979\n",
      "Epoch 40/150, Loss: 0.2052\n",
      "Epoch 40/150, Loss: 0.2052\n",
      "Epoch 41/150, Loss: 0.1860\n",
      "Epoch 41/150, Loss: 0.1860\n",
      "Epoch 42/150, Loss: 0.1847\n",
      "Epoch 42/150, Loss: 0.1847\n",
      "Epoch 43/150, Loss: 0.1725\n",
      "Epoch 43/150, Loss: 0.1725\n",
      "Epoch 44/150, Loss: 0.1598\n",
      "Epoch 44/150, Loss: 0.1598\n",
      "Epoch 45/150, Loss: 0.1525\n",
      "Epoch 45/150, Loss: 0.1525\n",
      "Epoch 46/150, Loss: 0.1402\n",
      "Epoch 46/150, Loss: 0.1402\n",
      "Epoch 47/150, Loss: 0.1497\n",
      "Epoch 47/150, Loss: 0.1497\n",
      "Epoch 48/150, Loss: 0.1297\n",
      "Epoch 48/150, Loss: 0.1297\n",
      "Epoch 49/150, Loss: 0.1124\n",
      "Epoch 49/150, Loss: 0.1124\n",
      "Epoch 50/150, Loss: 0.1042\n",
      "Epoch 50/150, Loss: 0.1042\n",
      "Epoch 51/150, Loss: 0.1015\n",
      "Epoch 51/150, Loss: 0.1015\n",
      "Epoch 52/150, Loss: 0.0877\n",
      "Epoch 52/150, Loss: 0.0877\n",
      "Epoch 53/150, Loss: 0.0845\n",
      "Epoch 53/150, Loss: 0.0845\n",
      "Epoch 54/150, Loss: 0.0748\n",
      "Epoch 54/150, Loss: 0.0748\n",
      "Epoch 55/150, Loss: 0.0650\n",
      "Epoch 55/150, Loss: 0.0650\n",
      "Epoch 56/150, Loss: 0.0638\n",
      "Epoch 56/150, Loss: 0.0638\n",
      "Epoch 57/150, Loss: 0.0605\n",
      "Epoch 57/150, Loss: 0.0605\n",
      "Epoch 58/150, Loss: 0.0435\n",
      "Epoch 58/150, Loss: 0.0435\n",
      "Epoch 59/150, Loss: 0.0385\n",
      "Epoch 59/150, Loss: 0.0385\n",
      "Epoch 60/150, Loss: 0.0318\n",
      "Epoch 60/150, Loss: 0.0318\n",
      "Epoch 61/150, Loss: 0.0359\n",
      "Epoch 61/150, Loss: 0.0359\n",
      "Epoch 62/150, Loss: 0.0126\n",
      "Epoch 62/150, Loss: 0.0126\n",
      "Epoch 63/150, Loss: 0.0058\n",
      "Epoch 63/150, Loss: 0.0058\n",
      "Epoch 64/150, Loss: 0.0061\n",
      "Epoch 64/150, Loss: 0.0061\n",
      "Epoch 65/150, Loss: -0.0110\n",
      "Epoch 65/150, Loss: -0.0110\n",
      "Epoch 66/150, Loss: -0.0114\n",
      "Epoch 66/150, Loss: -0.0114\n",
      "Epoch 67/150, Loss: -0.0291\n",
      "Epoch 67/150, Loss: -0.0291\n",
      "Epoch 68/150, Loss: -0.0377\n",
      "Epoch 68/150, Loss: -0.0377\n",
      "Epoch 69/150, Loss: -0.0488\n",
      "Epoch 69/150, Loss: -0.0488\n",
      "Epoch 70/150, Loss: -0.0502\n",
      "Epoch 70/150, Loss: -0.0502\n",
      "Epoch 71/150, Loss: -0.0630\n",
      "Epoch 71/150, Loss: -0.0630\n",
      "Epoch 72/150, Loss: -0.0677\n",
      "Epoch 72/150, Loss: -0.0677\n",
      "Epoch 73/150, Loss: -0.0687\n",
      "Epoch 73/150, Loss: -0.0687\n",
      "Epoch 74/150, Loss: -0.0803\n",
      "Epoch 74/150, Loss: -0.0803\n",
      "Epoch 75/150, Loss: -0.1002\n",
      "Epoch 75/150, Loss: -0.1002\n",
      "Epoch 76/150, Loss: -0.1078\n",
      "Epoch 76/150, Loss: -0.1078\n",
      "Epoch 77/150, Loss: -0.1261\n",
      "Epoch 77/150, Loss: -0.1261\n",
      "Epoch 78/150, Loss: -0.1348\n",
      "Epoch 78/150, Loss: -0.1348\n",
      "Epoch 79/150, Loss: -0.1429\n",
      "Epoch 79/150, Loss: -0.1429\n",
      "Epoch 80/150, Loss: -0.1603\n",
      "Epoch 80/150, Loss: -0.1603\n",
      "Epoch 81/150, Loss: -0.1681\n",
      "Epoch 81/150, Loss: -0.1681\n",
      "Epoch 82/150, Loss: -0.1845\n",
      "Epoch 82/150, Loss: -0.1845\n",
      "Epoch 83/150, Loss: -0.2075\n",
      "Epoch 83/150, Loss: -0.2075\n",
      "Epoch 84/150, Loss: -0.2146\n",
      "Epoch 84/150, Loss: -0.2146\n",
      "Epoch 85/150, Loss: -0.2301\n",
      "Epoch 85/150, Loss: -0.2301\n",
      "Epoch 86/150, Loss: -0.2492\n",
      "Epoch 86/150, Loss: -0.2492\n",
      "Epoch 87/150, Loss: -0.2665\n",
      "Epoch 87/150, Loss: -0.2665\n",
      "Epoch 88/150, Loss: -0.2789\n",
      "Epoch 88/150, Loss: -0.2789\n",
      "Epoch 89/150, Loss: -0.2958\n",
      "Epoch 89/150, Loss: -0.2958\n",
      "Epoch 90/150, Loss: -0.3163\n",
      "Epoch 90/150, Loss: -0.3163\n",
      "Epoch 91/150, Loss: -0.3430\n",
      "Epoch 91/150, Loss: -0.3430\n",
      "Epoch 92/150, Loss: -0.3512\n",
      "Epoch 92/150, Loss: -0.3512\n",
      "Epoch 93/150, Loss: -0.3722\n",
      "Epoch 93/150, Loss: -0.3722\n",
      "Epoch 94/150, Loss: -0.3990\n",
      "Epoch 94/150, Loss: -0.3990\n",
      "Epoch 95/150, Loss: -0.4061\n",
      "Epoch 95/150, Loss: -0.4061\n",
      "Epoch 96/150, Loss: -0.4297\n",
      "Epoch 96/150, Loss: -0.4297\n",
      "Epoch 97/150, Loss: -0.4431\n",
      "Epoch 97/150, Loss: -0.4431\n",
      "Epoch 98/150, Loss: -0.4475\n",
      "Epoch 98/150, Loss: -0.4475\n",
      "Epoch 99/150, Loss: -0.4650\n",
      "Epoch 99/150, Loss: -0.4650\n",
      "Epoch 100/150, Loss: -0.4739\n",
      "Epoch 100/150, Loss: -0.4739\n",
      "Epoch 101/150, Loss: -0.4878\n",
      "Epoch 101/150, Loss: -0.4878\n",
      "Epoch 102/150, Loss: -0.4994\n",
      "Epoch 102/150, Loss: -0.4994\n",
      "Epoch 103/150, Loss: -0.5107\n",
      "Epoch 103/150, Loss: -0.5107\n",
      "Epoch 104/150, Loss: -0.5113\n",
      "Epoch 104/150, Loss: -0.5113\n",
      "Epoch 105/150, Loss: -0.5224\n",
      "Epoch 105/150, Loss: -0.5224\n",
      "Epoch 106/150, Loss: -0.5250\n",
      "Epoch 106/150, Loss: -0.5250\n",
      "Epoch 107/150, Loss: -0.5329\n",
      "Epoch 107/150, Loss: -0.5329\n",
      "Epoch 108/150, Loss: -0.5392\n",
      "Epoch 108/150, Loss: -0.5392\n",
      "Epoch 109/150, Loss: -0.5435\n",
      "Epoch 109/150, Loss: -0.5435\n",
      "Epoch 110/150, Loss: -0.5531\n",
      "Epoch 110/150, Loss: -0.5531\n",
      "Epoch 111/150, Loss: -0.5560\n",
      "Epoch 111/150, Loss: -0.5560\n",
      "Epoch 112/150, Loss: -0.5600\n",
      "Epoch 112/150, Loss: -0.5600\n",
      "Epoch 113/150, Loss: -0.5613\n",
      "Epoch 113/150, Loss: -0.5613\n",
      "Epoch 114/150, Loss: -0.5644\n",
      "Epoch 114/150, Loss: -0.5644\n",
      "Epoch 115/150, Loss: -0.5699\n",
      "Epoch 115/150, Loss: -0.5699\n",
      "Epoch 116/150, Loss: -0.5801\n",
      "Epoch 116/150, Loss: -0.5801\n",
      "Epoch 117/150, Loss: -0.5806\n",
      "Epoch 117/150, Loss: -0.5806\n",
      "Epoch 118/150, Loss: -0.5894\n",
      "Epoch 118/150, Loss: -0.5894\n",
      "Epoch 119/150, Loss: -0.5935\n",
      "Epoch 119/150, Loss: -0.5935\n",
      "Epoch 120/150, Loss: -0.5984\n",
      "Epoch 120/150, Loss: -0.5984\n",
      "Epoch 121/150, Loss: -0.5912\n",
      "Epoch 121/150, Loss: -0.5912\n",
      "Epoch 122/150, Loss: -0.6024\n",
      "Epoch 122/150, Loss: -0.6024\n",
      "Epoch 123/150, Loss: -0.5989\n",
      "Epoch 123/150, Loss: -0.5989\n",
      "Epoch 124/150, Loss: -0.5996\n",
      "Epoch 124/150, Loss: -0.5996\n",
      "Epoch 125/150, Loss: -0.6121\n",
      "Epoch 125/150, Loss: -0.6121\n",
      "Epoch 126/150, Loss: -0.6102\n",
      "Epoch 126/150, Loss: -0.6102\n",
      "Epoch 127/150, Loss: -0.6141\n",
      "Epoch 127/150, Loss: -0.6141\n",
      "Epoch 128/150, Loss: -0.6086\n",
      "Epoch 128/150, Loss: -0.6086\n",
      "Epoch 129/150, Loss: -0.6189\n",
      "Epoch 129/150, Loss: -0.6189\n",
      "Epoch 130/150, Loss: -0.6184\n",
      "Epoch 130/150, Loss: -0.6184\n",
      "Epoch 131/150, Loss: -0.6251\n",
      "Epoch 131/150, Loss: -0.6251\n",
      "Epoch 132/150, Loss: -0.6259\n",
      "Epoch 132/150, Loss: -0.6259\n",
      "Epoch 133/150, Loss: -0.6237\n",
      "Epoch 133/150, Loss: -0.6237\n",
      "Epoch 134/150, Loss: -0.6286\n",
      "Epoch 134/150, Loss: -0.6286\n",
      "Epoch 135/150, Loss: -0.6412\n",
      "Epoch 135/150, Loss: -0.6412\n",
      "Epoch 136/150, Loss: -0.6406\n",
      "Epoch 136/150, Loss: -0.6406\n",
      "Epoch 137/150, Loss: -0.6389\n",
      "Epoch 137/150, Loss: -0.6389\n",
      "Epoch 138/150, Loss: -0.6360\n",
      "Epoch 138/150, Loss: -0.6360\n",
      "Epoch 139/150, Loss: -0.6470\n",
      "Epoch 139/150, Loss: -0.6470\n",
      "Epoch 140/150, Loss: -0.6474\n",
      "Epoch 140/150, Loss: -0.6474\n",
      "Epoch 141/150, Loss: -0.6501\n",
      "Epoch 141/150, Loss: -0.6501\n",
      "Epoch 142/150, Loss: -0.6487\n",
      "Epoch 142/150, Loss: -0.6487\n",
      "Epoch 143/150, Loss: -0.6460\n",
      "Epoch 143/150, Loss: -0.6460\n",
      "Epoch 144/150, Loss: -0.6568\n",
      "Epoch 144/150, Loss: -0.6568\n",
      "Epoch 145/150, Loss: -0.6576\n",
      "Epoch 145/150, Loss: -0.6576\n",
      "Epoch 146/150, Loss: -0.6603\n",
      "Epoch 146/150, Loss: -0.6603\n",
      "Epoch 147/150, Loss: -0.6568\n",
      "Epoch 147/150, Loss: -0.6568\n",
      "Epoch 148/150, Loss: -0.6630\n",
      "Epoch 148/150, Loss: -0.6630\n",
      "Epoch 149/150, Loss: -0.6610\n",
      "Epoch 149/150, Loss: -0.6610\n",
      "Epoch 150/150, Loss: -0.6712\n",
      "Epoch 150/150, Loss: -0.6712\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
    "u_dim = u_sequences[0].shape[0]\n",
    "C = x_sequences[0].shape[0]\n",
    "num_clusters = 3\n",
    "hidden0 = 64\n",
    "hidden1 = 32\n",
    "\n",
    "model = VAE_HMM(input_dim=C, hidden_dim=hidden0, K=num_clusters, hidden_dim2=hidden1, u_dim=u_dim, trans_hidden=64)\n",
    "trained = train_model(model, dataloader, num_epochs=150, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc1c586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder path after training\n",
    "encoder_path = 'encoder_saved.pth'\n",
    "torch.save({'model_state_dict': trained.encoder.state_dict(),\n",
    "            'config': {\n",
    "                'input_dim': C,\n",
    "                'hidden_dim': hidden0,\n",
    "                'hidden_dim2': hidden1,\n",
    "                'K': num_clusters\n",
    "            }\n",
    "           }, encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b77c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape (K, T): (5, 100)\n",
      "regimes.shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "end_day = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x_data = pd.read_csv('test_data.csv').drop(columns=['Date']).values[:end_day]\n",
    "u_data = pd.read_csv('test_data.csv').drop(columns=['Date', 'historical_vol']).values[:end_day]\n",
    "\n",
    "ckpt = torch.load('encoder_saved.pth', map_location='cpu')\n",
    "enc_cfg = ckpt.get('config', {})\n",
    "input_dim = enc_cfg.get('input_dim', C)\n",
    "hidden_dim = enc_cfg.get('hidden_dim', 32)\n",
    "hidden_dim2 = enc_cfg.get('hidden_dim2', hidden_dim)\n",
    "K = enc_cfg.get('K', num_clusters)\n",
    "\n",
    "# Recreate encoder architecture\n",
    "encoder_loaded = Encoder(input_dim, hidden_dim, hidden_dim2, K)\n",
    "encoder_loaded.load_state_dict(ckpt['model_state_dict'])\n",
    "encoder_loaded.to(device)\n",
    "encoder_loaded.eval()\n",
    "\n",
    "x_tensor = torch.tensor(x_data, dtype=torch.float).permute(1, 0).unsqueeze(0)  # (1, C, T)\n",
    "x_tensor = x_tensor.to(device)\n",
    "\n",
    "encoder_loaded.to(device)\n",
    "encoder_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    logits = encoder_loaded(x_tensor)# (1, K, T)\n",
    "    probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()# (K, T)\n",
    "    regimes = np.argmax(probs, axis=0)# (T,)\n",
    "\n",
    "print('probs.shape (K, T):', probs.shape)\n",
    "print('regimes.shape:', regimes.shape)\n",
    "\n",
    "# align with SP500 history (example)\n",
    "import yfinance as yf\n",
    "sp500 = yf.Ticker(\"^GSPC\")\n",
    "sp500_data = sp500.history(start=\"2020-01-01\", end=\"2024-12-31\")\n",
    "prices = sp500_data['Close'].values[:end_day]\n",
    "\n",
    "\n",
    "prices_aligned = prices[:len(regimes)]\n",
    "index_aligned = sp500_data.index[:len(regimes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1421d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatorai25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
